// ğŸ“ lib/services/llm_service.dart (ì‹ ê·œ íŒŒì¼)
// ë³µë¶™í•  ë–„ API í‚¤ ìœ ì¶œ ì£¼ì˜!

import 'dart:convert';
import 'dart:io';
import 'package:http/http.dart' as http;

class LlmService {
  // â­ï¸ 2ë‹¨ê³„ì—ì„œ ë°œê¸‰ë°›ì€ ë³¸ì¸ì˜ OpenRouter API í‚¤ë¥¼ ì—¬ê¸°ì— ë¶™ì—¬ë„£ìœ¼ì„¸ìš”
  static const String _apiKey =
      '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';

  static const String _apiUrl = 'https://openrouter.ai/api/v1/chat/completions';

  // í…ìŠ¤íŠ¸ ì§ˆë¬¸ -> í…ìŠ¤íŠ¸ ë‹µë³€ (ì˜ìƒ 0:40)
  static Future<String> getChatResponse(String prompt) async {
    try {
      final response = await http.post(
        Uri.parse(_apiUrl),
        headers: {
          'Authorization': 'Bearer $_apiKey',
          'Content-Type': 'application/json',
        },
        body: jsonEncode({
          // â­ï¸ í…ìŠ¤íŠ¸ìš© ë¬´ë£Œ ëª¨ë¸ (Mistral 7B)
          'model': 'mistralai/mistral-7b-instruct:free',
          'messages': [
            {'role': 'user', 'content': prompt}
          ],
        }),
      );

      if (response.statusCode == 200) {
        final data = jsonDecode(utf8.decode(response.bodyBytes));
        return data['choices'][0]['message']['content'];
      } else {
        print('LLM Error: ${response.statusCode}');
        print('LLM Body: ${response.body}');
        return 'Error: Could not get response from LLM.';
      }
    } catch (e) {
      print('LLM Exception: $e');
      return 'Error: $e';
    }
  }

  // ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ì§ˆë¬¸ -> í…ìŠ¤íŠ¸ ë‹µë³€ (ì˜ìƒ 1:12) [cite: 38]
  // (ì˜ìƒ 1:15ì˜ í•˜ë‹¨ í”„ë¦¬ë·°, 1:28ì˜ LLM ì‘ë‹µ ê´€ë ¨)
  static Future<String> getVisionResponse(File imageFile, String prompt) async {
    try {
      // 1. ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©
      final List<int> imageBytes = await imageFile.readAsBytes();
      final String base64Image = base64Encode(imageBytes);

      // 2. ì´ë¯¸ì§€ MIME íƒ€ì… í™•ì¸ (ê°„ë‹¨í•˜ê²Œ)
      final String mimeType =
          imageFile.path.endsWith('.png') ? 'image/png' : 'image/jpeg';

      final response = await http.post(
        Uri.parse(_apiUrl),
        headers: {
          'Authorization': 'Bearer $_apiKey',
          'Content-Type': 'application/json',
        },
        body: jsonEncode({
          // â­ï¸ ìš”ì²­í•˜ì‹  Vision ëª¨ë¸ (Gemini Pro Vision, ë¬´ë£Œ í‹°ì–´)
          // (NVIDIA ëª¨ë¸ì€ ìœ ë£Œì´ê±°ë‚˜ API í˜•ì‹ì´ ë‹¤ë¥¼ ìˆ˜ ìˆì–´,
          // í™•ì‹¤í•˜ê²Œ ì‘ë™í•˜ëŠ” ë¬´ë£Œ ëª¨ë¸ì¸ Geminië¡œ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤.)
          'model': 'google/gemini-pro-vision',
          'messages': [
            {
              'role': 'user',
              'content': [
                {'type': 'text', 'text': prompt},
                {
                  'type': 'image_url',
                  'image_url': {
                    'url': 'data:$mimeType;base64,$base64Image',
                  },
                },
              ],
            }
          ],
          'max_tokens': 300, // Vision ëª¨ë¸ì€ ì‘ë‹µ ê¸¸ì´ë¥¼ ì •í•´ì£¼ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
        }),
      );

      if (response.statusCode == 200) {
        final data = jsonDecode(utf8.decode(response.bodyBytes));
        return data['choices'][0]['message']['content'];
      } else {
        print('LLM (Vision) Error: ${response.statusCode}');
        print('LLM (Vision) Body: ${response.body}');
        return 'Error: Could not get response from LLM.';
      }
    } catch (e) {
      print('LLM (Vision) Exception: $e');
      return 'Error: $e';
    }
  }
}
