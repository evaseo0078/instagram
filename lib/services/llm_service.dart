// ğŸ“ lib/services/llm_service.dart

import 'dart:convert';
import 'dart:io';
import 'package:http/http.dart' as http;
import 'package:flutter_dotenv/flutter_dotenv.dart';

class LlmService {
  // .envì—ì„œ API í‚¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤
  static String get _apiKey => dotenv.env['OPENROUTER_API_KEY'] ?? '';

  static const String _apiUrl = 'https://openrouter.ai/api/v1/chat/completions';

  // â­ï¸ í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜ (ì´ìƒí•œ ê¸°í˜¸ ì œê±°)
  static String _cleanResponse(String text) {
    return text
        .replaceAll('<s>', '') // ì‹œì‘ íƒœê·¸ ì œê±°
        .replaceAll('</s>', '') // ì¢…ë£Œ íƒœê·¸ ì œê±°
        .replaceAll('[/s]', '') // ì´ìƒí•œ ì¢…ë£Œ íƒœê·¸ ì œê±°
        .replaceAll('[OUT]', '') // ì¶œë ¥ íƒœê·¸ ì œê±°
        .replaceAll('[/OUT]', '') // ì¶œë ¥ ì¢…ë£Œ íƒœê·¸ ì œê±°
        .replaceAll(RegExp(r'<.*?>'), '') // í˜¹ì‹œ ëª¨ë¥¼ ë‹¤ë¥¸ ê´„í˜¸ íƒœê·¸ ì œê±°
        .trim(); // ì•ë’¤ ê³µë°± ì œê±°
  }

  // í…ìŠ¤íŠ¸ ì§ˆë¬¸ -> í…ìŠ¤íŠ¸ ë‹µë³€
  static Future<String> getChatResponse(String prompt) async {
    try {
      final response = await http.post(
        Uri.parse(_apiUrl),
        headers: {
          'Authorization': 'Bearer $_apiKey',
          'Content-Type': 'application/json',
        },
        body: jsonEncode({
          // â­ï¸ ëª¨ë¸ì„ ì¡°ê¸ˆ ë” ì•ˆì •ì ì¸ Llama 3ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤ (ì„ íƒì‚¬í•­)
          // ê¸°ì¡´: 'mistralai/mistral-7b-instruct:free',
          'model': 'mistralai/mistral-7b-instruct:free',
          'messages': [
            {'role': 'user', 'content': prompt}
          ],
        }),
      );

      if (response.statusCode == 200) {
        final data = jsonDecode(utf8.decode(response.bodyBytes));
        String rawContent = data['choices'][0]['message']['content'];

        // â­ï¸ ì—¬ê¸°ì„œ ì •ì œ í•¨ìˆ˜ë¥¼ í†µí•´ ê¹¨ë—í•œ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜
        return _cleanResponse(rawContent);
      } else {
        print('LLM Error: ${response.statusCode}');
        print('LLM Body: ${response.body}');
        return 'Error: Could not get response from LLM.';
      }
    } catch (e) {
      print('LLM Exception: $e');
      return 'Error: $e';
    }
  }

  // ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ì§ˆë¬¸ -> í…ìŠ¤íŠ¸ ë‹µë³€
  static Future<String> getVisionResponse(File imageFile, String prompt) async {
    try {
      final List<int> imageBytes = await imageFile.readAsBytes();
      final String base64Image = base64Encode(imageBytes);

      final String mimeType =
          imageFile.path.endsWith('.png') ? 'image/png' : 'image/jpeg';

      final response = await http.post(
        Uri.parse(_apiUrl),
        headers: {
          'Authorization': 'Bearer $_apiKey',
          'Content-Type': 'application/json',
        },
        body: jsonEncode({
          'model': 'google/gemini-pro-vision',
          'messages': [
            {
              'role': 'user',
              'content': [
                {'type': 'text', 'text': prompt},
                {
                  'type': 'image_url',
                  'image_url': {
                    'url': 'data:$mimeType;base64,$base64Image',
                  },
                },
              ],
            }
          ],
          'max_tokens': 300,
        }),
      );

      if (response.statusCode == 200) {
        final data = jsonDecode(utf8.decode(response.bodyBytes));
        String rawContent = data['choices'][0]['message']['content'];

        // â­ï¸ ì—¬ê¸°ë„ ì •ì œ í•¨ìˆ˜ ì ìš©
        return _cleanResponse(rawContent);
      } else {
        print('LLM (Vision) Error: ${response.statusCode}');
        print('LLM (Vision) Body: ${response.body}');
        return 'Error: Could not get response from LLM.';
      }
    } catch (e) {
      print('LLM (Vision) Exception: $e');
      return 'Error: $e';
    }
  }
}
